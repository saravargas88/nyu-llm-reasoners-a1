{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ffd75",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp_env (Python 3.5.6)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n nlp_env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from einops import rearrange, einsum\n",
    "\n",
    "## Basic implementation\n",
    "Y = D @ A.T\n",
    "\n",
    "\n",
    "# Hard to tell the input and output shapes and what they mean.\n",
    "# What shapes can D and A have, and do any of these have unexpected behavior?\n",
    "## Einsum is self-documenting and robust\n",
    "# D A -> Y\n",
    "\n",
    "Y = einsum(D, A, \"batch sequence d_in, d_out d_in -> batch sequence d_out\")\n",
    "\n",
    "\n",
    "## Or, a batched version where D can have any leading dimensions but A is constrained.\n",
    "\n",
    "Y = einsum(D, A, \"... d_in, d_out d_in -> ... d_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Einsum notation can handle arbitrary input batching dimensions, but also has the key benefit of being\n",
    "self-documenting. Itâ€™s much clearer what the relevant shapes of your input and output tensors are in code\n",
    "that uses einsum notation. For the remaining tensors, you can consider using Tensor type hints, for instance jaxtyping \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
